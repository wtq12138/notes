# CAP

分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容忍性（P：Partition Tolerance），最多只能同时满足其中两项。

# Raft

## 状态以及RPC ppt

<img src="https://pic2.zhimg.com/80/v2-713752414ba7ff854f033fadbba42d7d_720w.jpg" alt="img" style="zoom:80%;" />

currentTerm  任期 (递增版本号 和paxos中的prepare异曲同工)

votedfor 一个键值对<k,v> k是term，v是term投的candidateID

commitIndex 集群中已写入大多数log的最高log数组的Index

lastApplied 已执行commend的最高log数组的Index

leader上的易失性状态，在选举之后重新初始化

- nextIndex[]：需要发送给每个服务器下一条日志条目索引号(初始化为leader的最高索引号+1)
- matchIndex[]：已复制到每个服务器上的最高日志条目号 已发送日志作用是GC

![img](https://pic3.zhimg.com/80/v2-2529d5a62780cf8b345a144eff35e57a_720w.jpg)

requestVote(term,candidateId,lastLogIndex,lastLogTerm) 

follower每个term只能投一票

**携带lastlog确保candidate的日志至少和follower的日志一样新** 

具体实现是candidate的最后一个日志 term大 或者 term相等但是日志更多 index大

投票RPC的作用是选举leader并且保证leader拥有最新的日志，因为大多数回复才能写入日志，所以只有最新的一部分才能得到大多数选票，而少部分旧的是无法得到最新的这大部分的选票，也就不会成为leader

<img src="https://pic3.zhimg.com/80/v2-30191f5f950dd9d13f298f2d558b8e42_720w.jpg" alt="img" style="zoom:80%;" />

appendEntry(term,leaderId,prevLogIndex,prevLogTerm,entry,leaderCommit)

**prevLogIndex,prevLogTerm的作用是为了保证follower日志最新日志与leader新写入的日志的前一个日志是一致同步的**

<img src="https://pic4.zhimg.com/80/v2-b39ec08e180291728ef138c0aaef184f_720w.jpg" alt="img" style="zoom:80%;" />



## 转换状态图

<img src="https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d9f2d1f2d3674a50900eda504ecaa326~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp" alt="img" style="zoom: 50%;" />

## term的几种情况

timeout 后follower变为candidate term会++

节点接收到其他节点的RPC发现自己的term较小，会将自己的term更新为较大值，如果此时是candidate或者leader会立即变为follower

如果发现其他节点的RPC中携带的term较小，则会直接拒绝请求



## 重要定理

**在同一个TermId最多只有一个leader写入**

原因：大多数原则

**对于两个节点同一个index 如果term相同则index前的日志一定相同** 

原因：leader发起appendEntry  RPC会携带前一个prelog的term  如果follower的prelog不相同会进行覆盖 这样保证了follower和leader的一致性

**leade base    选举一定是在拥有最新的commit日志的几个节点中选举，保证leader拥有所有commited的日志 **

原因：candidate发起vote  RPC会携带最后一个log，保证candidate的最后一个日志 term大 或者 term相等但是日志更多 index大

**不会commit上一轮term的log，只能commit当前term的log**

原因： 在（a）中，S1是leader，并且向其他服务器部分复制了索引2处的日志条目。 在（b）中，S1崩溃； S5被选为任期为3的leader，S3，S4以及他自己投的票，并且在索引为2的地方接受了不同的条目。在（c）中S5崩溃; S1重新启动，被选为leader，并继续复制。 此时，任期2中的日志条目已在大多数服务器上复制，但是尚未提交。 如果S1像（d）中那样崩溃，之后S5被选为leader（来自S2，S3和S4的投票），并用任期3中的条目覆盖该条目。但是，如果S1在宕机之前从当前任期把日志条目复制给了大多数服务器，如（e），那么S5的条目2是无法提交的(S5无法赢得选举)。 此时，日志中的所有先前条目也将被提交。

针对上述情况就是：即使日志（2，2）已经被大多数节点（S1、S2、S3）确认了，但是它不能被Commit，因为它是来自之前term(2)的日志，直到S1在当前term（4）产生的日志（4， 3）被大多数Follower确认，S1方可Commit（4，3）这条日志，当然，根据Raft定义，（4，3）之前的所有日志也会被Commit。此时即使S1再下线，重新选主时S5不可能成为Leader，因为它没有包含大多数节点已经拥有的日志（4，3）。

<img src="https://pic3.zhimg.com/80/v2-38aa2d44b0d2aec933cc30b6b7e7faa2_720w.jpg" alt="img" style="zoom:80%;" />



## 流程 

每个节点三种状态leader follower candidate

经历两个过程leader election和log replication

**leader election** 

初始均为follower，每个节点在无leader时都会自旋一个随机时间election timeout，时间一到就会成为candidate  term++ 开始election

candidate  向所有节点发送RPC vote(term,id)表示在term任期给id投票

收到RPC的follower 会取term=max(request.term,term)    如果request.term小的话直接拒绝   否则根据term是否已经投票，决定是否为candidate投票

并刷新 election timeout，candidate变为leader后，向所有follower发送heartbeat，携带log进行数据一致操作即log replication

heartbeat时有同样有一个 timeout，如果leader挂掉 ，仍然会出现一个candidate，开始election并成为leader 重复上述

如果出现多个candidate，投票后票数一样的话,会重新进行election timeout,得到candidate 开始election 



三种情况

- 它赢得了选举；

收到了大多数vote

- 另一台服务器赢得了选举；

等待别人vote时收到了leader的 AppendEntries RPC(heartbeat) 根据term大小来选择拒绝RPC保持candidate状态还是接受RPC变为Follower

- 一段时间后没有任何一台服务器赢得了选举。

因为随机时间保证之后不会一直重新选举



**log replication**

所有操作必须收到回复后才会执行

客户端请求会先发往leader, leader将请求日志添加log，在下一次向所有follower发送heartbeat时发送appendEntry RPC请求

follower收到log但不commit而是返回leader消息，leader收到大部分回复后，commit并向客户端回复确认，

follower在下一个headbeat收到消息时，才执行上步的log（为什么？）

ps:我的理解： leader存日志到本地，发送heartbeat携带log,而follower收到log不能立刻执行，因为follower的response无法判断是否会到达leader

必须等下一次heartbeat收到消息，才能证明通信正常而且根据log号可知道之前的logleader是否已经commited

当故障出现分区时，每个分区会出现一个领导，而恢复后会选择一个election term轮次多的成为新领导，

之前较小的分区虽然有日志，但是无法执行，为了和新领导同步，需要回滚与其一致

## 日志恢复

Leader 针对每个 follower 都维护一个 **next index**，表示下一条需要发送给该follower 的日志索引。当一个 leader 刚刚上任时，它初始化所有 next index 值为自己最后一条日志的 index+1。但凡某个 follower 的日志跟 leader 不一致，那么下次 AppendEntries RPC 的一致性检查就会失败。在被 follower 拒绝这次 Append Entries RPC 后，leader 会减少 next index 的值并进行重试。

# Paxos

https://zhuanlan.zhihu.com/p/42912831

https://cloud.tencent.com/developer/article/1158799

引入

分布式复制策略

主从异步、同步、半同步

多数派写 读写均要求>=2/n+1

问题是并发修改时 无法防止客户端并发覆盖问题

尝试1 写前多数派读 检测是否已写过

问题 多数派读仍然可能并发



尝试2 存储节点还需要增加一个功能, 就是它必须记住谁最后一个做过**写前读取**的操作. 并且只允许最后一个完成**写前读取**的进程可以进行后续写入, 同时拒绝之前做过**写前读取**的进程写入的权限.

改进之后即为basic paxos

## basic paxos

**我的理解**

basic paxos在第一轮proposal中达成一致 在第二轮accept写入时 趋于上一次一致 只有第二个阶段accept后才算同步 第一个阶段是同步前的检查 

两阶段思想！

![img](https://pic2.zhimg.com/80/v2-0fb6fda91f543b6760c5329e82e7ad95_720w.jpg?source=1940ef5c)



![img](https://pic1.zhimg.com/80/v2-288515af9b4118e774bab93cb74f2857_720w.jpg?source=1940ef5c)



paxos的phase-1, 它相当于之前提到的写前读取过程.

![v2-5148b510c03ba42e1221001b2dec4e99_720w](F:\资料\八股复习\冲冲冲\分布式\images\v2-5148b510c03ba42e1221001b2dec4e99_720w.jpg)





![img](https://pica.zhimg.com/80/v2-57fb1a930d3c41c3725137a669dd7a40_720w.jpg?source=1940ef5c)

在第2阶段phase-2, Proposer X将它选定的值写入到Acceptor中, 这个值可能是它自己要写入的值, 或者是它从某个Acceptor上读到的v(修复共识).

![img](https://pic1.zhimg.com/80/v2-33233d9740cad6ca291711123592d69d_720w.jpg?source=1940ef5c)





![img](https://pic1.zhimg.com/80/v2-c050e21d9d3f12bb24150f8f19d102c8_720w.jpg?source=1940ef5c)



eg

![img](https://pic2.zhimg.com/80/v2-6472642b4a1ad5dc246c79d0229425b7_720w.jpg?source=1940ef5c)





继续上面的例子, 看X如何处理被抢走写入权的情况:

这时X的phase-2没成功, 它需要重新来一遍, 用更大的rnd=3.

- X成功在左边2个Acceptor上运行phase-1之后, X发现了2个被写入的值: v=x, vrnd=1 和 v=y, vrnd=2; 这时X就不能再写入自己想要写入的值了. 它这次paxos运行必须不能修改已存在的值, 这次X的paxos的运行唯一能做的就是, 修复(可能)已经中断的其他proposer的运行.
- 这里v=y, vrnd=2 是可能在phase-2达到多数派的值. v=x, vrnd=1不可能是, 因为其他proposer也必须遵守算法约定, 如果v=x, vrnd=1在某个phase-2达到多数派了, Y一定能在phase-1中看到它, 从而不会写入v=y, vrnd=2.(因为只有多数派才能写入所以vrnd=2在靠后的轮次写入的大多数)

因此这是X选择v=y, 并使用rnd=3继续运行, 最终把v=y, vrnd=3写入到所有Acceptor中.



![img](https://pic2.zhimg.com/80/v2-01aeaa06dd92aa08f1322db83d5446c1_720w.jpg?source=1940ef5c)



## multi paxos

basic paxos 的proposal阶段实际上在决定leader 开销较大 且存在活锁问题 且basic只能决定一个值的共识

multi paxos引入leader 只能由leader 进行proposal 优化Prepare阶段   可以决定多个值且保证顺序

因为要决定多个值可以并发请求所以每个值对应一个index 即数组下标，最后这些并发请求也是固定顺序存放在数组中





<img src="http://note.youdao.com/yws/api/personal/file/WEB98641411aadcb17d35a08061a889673d?method=download&shareKey=6e7507e372e6673074a31b156cf67251" alt="image" style="zoom: 25%;" />

这里的chosen是集群每一列大多数已经存储值 可以确定下来了



当 jmp 请求到达 S1 后，它会找到第一个没有被选定的记录（3-cmp），然后它会试图让 jmp 作为该记录的选定值。**为了让这个例子更具体一些，我们假设服务器 S3 已经下线。**所以 Paxos 协议在服务器 S1 和 S2 上运行，服务器 S1 会尝试让记录 3 接受 jmp 值，它正好发现记录 3 内已经有值（3-cmp），这时它会结束选择并进行比较，S1 会向 S2 发送接受（3-cmp）的请求，S2 会接受 S1 上已经接受的 3-cmp 并完成选定。这时（3-cmp）变成粗体，不过还没能完成客户端的请求，所以我们返回到第一步，重新进行选择。

找到当前还没有被选定的记录（这次是记录 4-），这时 S1 会发现 S2 相应记录上已经存在接受值（4-sub），所以它会再次放弃 jmp ，并将 sub 值作为它 4- 记录的选定值。所以此时 S1 会再次返回到第一步，重新进行选择，当前未被选定的记录是 5- ，这次它会成功的选定 5-jmp ，因为 S1 没有发现其他服务器上有接受值。

**如果S3 不下线的话** 对于第四列 如果 原本 s1 或s3 accept sub的速度比basic paxos jmp 的prepare请求要慢的话 会导致 在第四列 sub jmp达成共识

### 状态

**Acceptor 上的持久化状态**

| 参数                  | 解释                                                         |
| --------------------- | ------------------------------------------------------------ |
| lastLogIndex          | 已经accept的最大的日志index 日志数组可达的最大范围           |
| minProposal(last_rnd) | 将要收到的prepare中的最小proposal编号(见过的最大prepare的round)，如果还未收到 Prepare 请求，则为 0 |
| firstUnchosenIndex    | i > 0 且 acceptedProposal[i] < ∞ 的最小日志 index            |

每个 Acceptor 上还会存储一个日志数组，日志索引 i ∈ [1, lastLogIndex]，每条日志记录包含以下内容：

| 参数                   | 解释                                                         |
| ---------------------- | ------------------------------------------------------------ |
| acceptedProposal[i]    | 第 i 条日志最后接受的提案编号。初始化时为 0；如果提案被 chosen（半数accept），则 acceptedProposal[i] = 无穷大 |
| acceptedValue[i] value | 第 i 条日志最后接受的 value，初始化时为 null                 |

**Proposer 上的持久化状态**

| 参数     | 解释                     |
| -------- | ------------------------ |
| maxRound | Proposer 的 round number |

**Proposer 上的易失性状态**

| 参数      | 解释                                                         |
| --------- | ------------------------------------------------------------ |
| nextIndex | 客户端请求要写的下一个日志 index                             |
| prepared  | 如果 prepared 为 True，那么 Proposer 不再需要发起 Prepare 请求（超过半数的 Acceptor 回复了 noMoreAccepted）；初始化为 False |

### 为什么multi paxos比raft抽象很多

1. basic->multi

basic    prepare 阶段将单个index位置作为个体，而multi prepare阶段将整个log日志作为个体

prepare的目的是为了写前检查是否有更新的值来覆盖

从basic过渡到multi从检查单体变为了检查index后是否有写入

2. 日志同步

和raft一样都有一个在从leader->follower写日志时同步commitIndex来将follower的日志commit，同样只能提交自己的日志 即等于request.round的日志

而不一样的是这里follower会返回一个commitIndex，leader根据其log上的commit情况继续向其发起successRPC进行日志同步

3. raft流程简单

raft只分为选举leader election和log replication 且因为leader base的特性leader天然拥有最新的log，且在日志同步时不允许日志空洞，因为log replication时会携带preLog进行校验，这样的好处是新leader上任时较为简单

### RPC Message

####  Prepare（阶段 1）

**请求：**

- n：提案编号
- index：Proposer 的提案对应的日志 index

**接受者处理：**

收到 Prepare 请求后，如果 `request.n >= minProposal`，则 Acceptor 设置 `minProposal = request. proposalId`；同时承诺拒绝所有提案编号 < request.n 的 Accept 请求。

**响应：**

- acceptedProposal：Acceptor 的 `acceptedProposal[index]`
- acceptedValue：Acceptor 的 `acceptedValue[index]`
- **noMoreAccepted**：Acceptor 遍历 >= index 的日志记录，如果之后没有接受过任何值（都是空的记录），那么 noMoreAccepted = True；否则设为 False

##### 如何理解noMoreAccepted

basic的minproposal是对于每个log数组维护一个当前见过的prepare收到的最大round值

而multi的minproposal是对于整个log数组，而又因为log数组顺序一定是单增的 所以只要满足遍历 >= index 的日志记录，如果之后没有接受过任何值 即说明后面未曾接受过比当前大的proposal说明之后无写入

而prepare的目的是防止已达成共识的写入被覆盖 所以可以省去prepare这一步

#### Accept（阶段 2）

**请求：**

- n：和 Prepare 阶段一样的提案编号
- index：日志 index
- v：提案的值，如果 Prepare 阶段收到一个更大的提案编号，那么就是该最大的提案的值，否则 Proposer 使用来自 Client 的值
- firstUnchosenIndex：节点日志上第一个没有被 chosen 的日志 index

**接受者处理：**

收到 Accept 请求后，如果 `n >= minProposal,` 则：

- `acceptedProposal[index] = n`

- `acceptedValue[index] = v`

- minProposal = n   

  对于每个 `index < request.firstUnchosenIndex`，如果 `acceptedProposal[index] = n`，则 `acceptedProposal[index] = ∞`

​		这个根据现在的proposer的chosen情况 将其同步到accpetor

**响应：**

- n：Acceptor 的 minProposal 值
- firstUnchosenIndex：Acceptor 的 firstUnchosenIndex 值

##### 如何理解日志同步

如果proposer的firstUnchosenIndex比acceptor的firstUnchosenIndex大的话，说明acceptor还有部分日志未同步，如果acceptor与proposer round轮次相同表明需要同步

注意 leader 只提交自己生成的日志，不能代替之前的 leader 提交日志

#### Success（阶段 3）

**请求：**

- index：日志的索引
- v：log[index] 已 chosen 的提案值

**接受者处理：**

Acceptor 收到 Success RPC 后，更新已经被 chosen 的日志记录：

- acceptedValue[index] = v
- acceptedProposal[index] = 无穷大

**响应：**

- firstUnchosenIndex：Acceptor 的 firstUnchosenIndex 值

当发送者收到响应后，如果 `reply.firstUnchosenIndex < firstUnchosenIndex`，则发送者再发生请求：`Success(index = reply.firstUnchosenIndex, value = acceptedValue[reply.firstUnchosenIndex])`

##### 如何理解success

因为leader收到的acceptedProposal[reply.firstUnchosenIndex] == ∞说明已经chosen，但是这个消息不一定其他节点知晓，需要将已chosen的值写到其他节点上面

lear上提交的日志一定有效

#### 流程

1. 如果不是 Leader，或者 Leader 还没有初始化完成，直接返回 False

2. 如果 prepared == True：

- index = nextIndex, nextIndex++
- goto 7

3. index = firstUnchosenIndex，nextIndex = index + 1

4. 生成一个新的提案编号 n（maxRound++，并持久化保存）

5. 广播 `Prepare(n, index)` 给所有 Acceptor

6. 一旦收到超过半数 Acceptor 的 Prepare 响应（`reply.acceptedProposal,reply.acceptedValue,reply.noMoreAccepted`）：

- 如果所有响应中最大的 `reply.acceptedProposal` 不等于 0，那么使用它的 `reply.acceptedValue`，否则使用自己的 `inputValue`
- 如果超过半数的 Acceptor 回复了 `reply.noMoreAccepted = True`，那么 `prepared = true`

7. 广播 `Accept(index, n, v)` 到所有的 Acceptor

8. 一旦收到一个 Acceptor 的响应（`reply.n, reply.firstUnchosenIndex`）

- 如果 `reply.n > n`，则从 `reply.n` 中修改 `maxRound`，修改 `prepared = False`，**跳转到 1**
- 如果 `reply.firstUnchosenIndex ≤ lastLogIndex` 并且 `acceptedProposal[reply.firstUnchosenIndex] == ∞`，就发送 `Success(index = reply.firstUnchosenIndex, value = acceptedValue[reply.firstUnchosenIndex])`

9. 一旦收到超过半数 Acceptor 的 Accept 响应：修改 `acceptedProposal[index] = ∞` 和 `acceptedValue[index] = v`
10. 如果 `v == inputValue`， 返回 True
11. **跳转到 2**



# BASE理论

BASE 是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）

降级  中间状态

# 熔断降级限流

熔断指某服务挂掉，之后不调用它直接返回降级数据

降级指保留核心业务，其他业务直接返回降级数据

# 解决方案

## 2PC

两阶段提交（Two-phase Commit，2PC），通过引入协调者（Coordinator）来协调参与者的行为，并最终决定这些参与者是否要真正执行事务

## 柔性事务

尽力去通知但是不保证

# 常见分布式问题

## 脑裂

主节点p1被错误的认为已死，将从节点提升为主节点p2，这时一部分认为p1为主，一部分认为p2为主